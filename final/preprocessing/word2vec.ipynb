{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, csv, random, re, nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors successfully!\n"
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "print(\"Loaded word vectors successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set()\n",
    "with open('stops.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stop_words.add(line.strip())\n",
    "for w in stopwords.words('english'):\n",
    "    stop_words.add(w)\n",
    "total_train = 10000\n",
    "total_test = 2000\n",
    "\n",
    "train_idx = range(total_train)\n",
    "test_idx = range(total_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefixReplace(match):\n",
    "    prefix, stem = match.group(1), match.group(2)\n",
    "    temp = prefix + stem\n",
    "    if not stem in stop_words:\n",
    "        temp += ' ' + stem\n",
    "    return temp\n",
    "\n",
    "def hyphenReplace(match):\n",
    "    temp = match.group()\n",
    "    li = temp.split('-')\n",
    "    temp = temp.replace('-', '')\n",
    "    for item in li:\n",
    "        if not item in stop_words:\n",
    "            temp += ' ' + item\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    p_prefix = re.compile(r'\\b(a|an|ante|anti|auto|circum|co|com|con|contra|contro|de|dis|en|em|ex|extra|fore|hetero|homo|homeo|hyper|il|im|in|ir|inter|intra|intro|macro|micro|mid|mis|mono|non|omni|over|post|pre|pro|re|semi|sub|super|sym|syn|trans|tri|un|under|uni)-([a-z])+\\b', re.I)\n",
    "    p_hyphen = re.compile(r'\\b(\\w+-)+\\w+\\b')\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        li = []\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.lower()\n",
    "                # expand stem with hyphen prefix\n",
    "                line = p_prefix.sub(prefixReplace, line)\n",
    "                # expand hyphenated word\n",
    "                line = p_hyphen.sub(hyphenReplace, line)\n",
    "                line = line.replace(':', ' ')\n",
    "                line = line.replace('\\'s', '')\n",
    "                line = line.replace(',', ' ')\n",
    "                line = line.replace('.', ' ')\n",
    "                li += word_tokenize(line)\n",
    "                li = [wnl.lemmatize(x) for x in li]\n",
    "        docs.append(li)\n",
    "    return docs\n",
    "\n",
    "def doc_to_vec(li, word2vec):\n",
    "    # get list of word vectors in sentence\n",
    "    word_vecs = []\n",
    "    for w in li:\n",
    "        if w not in word2vec.vocab:\n",
    "#             print('not found:',w)\n",
    "            pass\n",
    "        elif w in stop_words:\n",
    "#             print('stop word:', w)\n",
    "            pass\n",
    "        else:\n",
    "            word_vecs.append(word2vec.get_vector(w))\n",
    "    if not li:\n",
    "        return np.zeros(300)\n",
    "    # return average\n",
    "    return np.stack(word_vecs).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all description matrices!\n",
      "d_train shape: (10000, 300)\n",
      "d_test shape: (2000, 300)\n"
     ]
    }
   ],
   "source": [
    "# build description matrices\n",
    "d_train_dev = parse_descriptions(\"../data/descriptions_train\", num_doc=total_train)\n",
    "test_desc = parse_descriptions(\"../data/descriptions_test\", num_doc=total_test)\n",
    "d_train = np.array([doc_to_vec(d_train_dev[i], word2vec) for i in train_idx])\n",
    "d_test = np.array([doc_to_vec(test_desc[i], word2vec) for i in test_idx])\n",
    "d_train_dev.clear()\n",
    "test_desc.clear()\n",
    "del d_train_dev, test_desc\n",
    "np.save('word2vec_train_description.npy', d_train)\n",
    "np.save('word2vec_test_description.npy', d_test)\n",
    "print(\"Built all description matrices!\")\n",
    "print(\"d_train shape:\", d_train.shape)\n",
    "print(\"d_test shape:\", d_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all tag matrices!\n",
      "t_train shape: (10000, 300)\n",
      "t_test shape: (2000, 300)\n"
     ]
    }
   ],
   "source": [
    "# build test matrices\n",
    "t_train_dev = parse_descriptions(\"../data/tags_train\", num_doc=total_train)\n",
    "test_tag = parse_descriptions(\"../data/tags_test\", num_doc=total_test)\n",
    "t_train = np.array([doc_to_vec(t_train_dev[i], word2vec) for i in train_idx])\n",
    "t_test = np.array([doc_to_vec(test_tag[i], word2vec) for i in test_idx])\n",
    "\n",
    "t_train_dev.clear()\n",
    "test_tag.clear()\n",
    "del t_train_dev, test_tag\n",
    "np.save('word2vec_train_tag.npy', t_train)\n",
    "np.save('word2vec_test_tag.npy', t_test)\n",
    "print(\"Built all tag matrices!\")\n",
    "print(\"t_train shape:\", t_train.shape)\n",
    "print(\"t_test shape:\", t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all feature vec matrices!\n",
      "f_train shape: (10000, 1000)\n",
      "f_test shape: (2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "def parse_features(features_path):\n",
    "    vec_map = {}\n",
    "    with open(features_path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            img_id = int(row[0].split(\"/\")[1].split(\".\")[0])\n",
    "            vec_map[img_id] = np.array([float(x) for x in row[1:]])\n",
    "    return np.array([v for k, v in sorted(vec_map.items())])\n",
    "\n",
    "# build feature matrices\n",
    "# p = np.random.randn(1000, 200)\n",
    "f_train_dev = parse_features(\"../data/features_train/features_resnet1000_train.csv\")# @ p\n",
    "f_train = f_train_dev[train_idx]\n",
    "f_test = parse_features(\"../data/features_test/features_resnet1000_test.csv\")# @ p\n",
    "f_test = f_test[test_idx]\n",
    "\n",
    "del f_train_dev\n",
    "np.save('word2vec_train_1000feature.npy', f_train)\n",
    "np.save('word2vec_test_1000feature.npy', f_test)\n",
    "print(\"Built all feature vec matrices!\")\n",
    "print(\"f_train shape:\", f_train.shape)\n",
    "print(\"f_test shape:\", f_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all intermediate feature vec matrices!\n",
      "fi_train shape: (10000, 2048)\n",
      "fi_test shape: (2000, 2048)\n"
     ]
    }
   ],
   "source": [
    "fi_train_dev = parse_features(\"../data/features_train/features_resnet1000intermediate_train.csv\")\n",
    "fi_train = fi_train_dev[train_idx]\n",
    "fi_test = parse_features(\"../data/features_test/features_resnet1000intermediate_test.csv\")# @ p\n",
    "fi_test = fi_test[test_idx]\n",
    "\n",
    "del fi_train_dev\n",
    "np.save('word2vec_train_2048feature.npy', fi_train)\n",
    "np.save('word2vec_test_2048feature.npy', fi_test)\n",
    "print(\"Built all intermediate feature vec matrices!\")\n",
    "print(\"fi_train shape:\", fi_train.shape)\n",
    "print(\"fi_test shape:\", fi_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
