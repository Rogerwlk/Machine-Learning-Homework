{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from IPython.display import Image\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import gensim\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\"./data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "print(\"Loaded word vectors successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of desc_vector: 1000\n",
      "desc_vector[0] is index; desc_vector[1] is vector\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "## get the vector representatation of description\n",
    "\n",
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        with open(path) as f:\n",
    "            docs.append([i,f.read()])\n",
    "    return docs\n",
    "\n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    # get list of word vectors in sentence\n",
    "    word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "    # return average\n",
    "    tmp = np.stack(word_vecs)\n",
    "    return tmp.mean(0)\n",
    "\n",
    "desc_train = parse_descriptions(\"./data/descriptions_train\", num_doc=num_train)\n",
    "desc_vector_train = []\n",
    "for i in range(len(desc_train)):\n",
    "    tmp = doc_to_vec(desc_train[i][1], word2vec)\n",
    "    #print(tmp)\n",
    "    desc_vector_train.append([desc_train[i][0],tmp])\n",
    "    \n",
    "print(\"the length of desc_vector:\",len(desc_vector_train))\n",
    "print(\"desc_vector[0] is index; desc_vector[1] is vector\")\n",
    "print(len(desc_vector_train[3][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of feature_vector_train is: 1000\n",
      "feature_vector_train[0] is index; feature_vector_train[1] is the vector\n",
      "[1.642971396446228, -0.5919864773750305, -2.0084922313690186, -1.6832419633865356, 0.1523834466934204, -0.5803146362304688, -2.876101493835449, -2.0010290145874023, -2.138638496398926, -0.8315469622612, -0.43401849269866943, -1.1083484888076782, -1.7107912302017212, -2.6106507778167725, 1.337751865386963, -1.6990853548049927, -2.0171265602111816, -1.9067765474319458, -1.197595238685608, -2.3860580921173096, 0.7725794911384583, 0.10497377067804337, 0.8021477460861206, -0.04659649357199669, -1.7470080852508545, -0.37388086318969727, -0.5517594814300537, -0.04499737173318863, -1.6107257604599, -0.6461459994316101, -2.223390817642212, -1.167757272720337, -0.374008446931839, -1.603108286857605, 0.22323819994926453, -1.2288230657577515, 0.560768187046051, -1.068021535873413, -1.039835810661316, -1.387953281402588, -2.279953956604004, -2.242109775543213, -2.6766748428344727, -3.5659289360046387, -1.0766853094100952, -1.4477565288543701, -2.7968859672546387, -0.06221980229020119, -2.7919273376464844, -1.7334407567977905, -1.5387991666793823, -0.7024900913238525, 0.586330771446228, -0.39349719882011414, -2.870737314224243, -0.8090109825134277, -3.9153378009796143, -1.6158796548843384, -0.14538460969924927, 0.39167478680610657, -1.0627365112304688, -2.438880443572998, -0.9430412650108337, 0.09169631451368332, 0.310448557138443, -0.8708688020706177, -1.6148583889007568, -0.3944753110408783, -2.1407830715179443, -3.1953213214874268, -0.4981923997402191, -2.24250864982605, -1.804477572441101, 0.6150521636009216, -2.4310455322265625, -3.1957085132598877, -2.497063636779785, -3.7617881298065186, 1.2911450862884521, -0.7535297274589539, 0.08491232991218567, -1.6663248538970947, -0.24488559365272522, -0.3770252466201782, -0.40562331676483154, -1.2372307777404785, 0.6419746279716492, 0.03832010552287102, -0.9582026600837708, 0.7292354702949524, -1.160616397857666, -3.98482346534729, -2.5989949703216553, -3.63988995552063, -0.9777348041534424, -1.1985595226287842, -1.6363584995269775, -1.8273828029632568, -1.4709734916687012, 1.6343272924423218]\n"
     ]
    }
   ],
   "source": [
    "## get the feature vector \n",
    "\n",
    "#read feature vectors\n",
    "f = open('./data/features_train/features_resnet1000_train.csv', 'r')\n",
    "\n",
    "dimension_of_feature_vector =100\n",
    "\n",
    "# i = 0;\n",
    "feature_vector_train = []\n",
    "for line in f:\n",
    "    tmp = line.strip().split(',')\n",
    "    index = int(tmp[0][13:-4])\n",
    "    vector = [float(i) for i in tmp[1:]]\n",
    "    feature_vector_train.append([index,vector[0:dimension_of_feature_vector]])\n",
    "    #print(feature_vector_train[-1])\n",
    "#     i=i+1\n",
    "#     if i>= num_train:\n",
    "#         break\n",
    "f.close()\n",
    "\n",
    "# sort feature_vector_train accroding to index\n",
    "def take_index(elem):\n",
    "    return[elem[0]]\n",
    "\n",
    "#print(feature_vector_train[3][0])\n",
    "feature_vector_train.sort(key=take_index)\n",
    "\n",
    "feature_vector_train = feature_vector_train[:num_train]\n",
    "\n",
    "print(\"the length of feature_vector_train is:\", len(feature_vector_train))\n",
    "print(\"feature_vector_train[0] is index; feature_vector_train[1] is the vector\")\n",
    "print(feature_vector_train[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################the length of feature_dist_train:  1000000\n",
      "[0, 3, 19.260680431729618]\n"
     ]
    }
   ],
   "source": [
    "## get the euclidean distance between feature vector of i and j\n",
    "## euclidean distance can work as the dissimilarity score\n",
    "\n",
    "feature_dist_train = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(feature_vector_train)):\n",
    "    for j in range(len(feature_vector_train)):\n",
    "        count = count +1;\n",
    "        if count == 10000:\n",
    "            print(\"#\",end='')\n",
    "            count = 0\n",
    "\n",
    "        dist = distance.euclidean(feature_vector_train[i][1],feature_vector_train[j][1])\n",
    "        #i,j,index\n",
    "        #print(i,j)\n",
    "        #print(feature_vector_train[i][0],feature_vector_train[j][0])\n",
    "        feature_dist_train.append([feature_vector_train[i][0],feature_vector_train[j][0],dist])\n",
    "\n",
    "print(\"the length of feature_dist_train: \", len(feature_dist_train))\n",
    "print(feature_dist_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of x_train is:  1000000\n"
     ]
    }
   ],
   "source": [
    "## get the x_train\n",
    "x_train = []\n",
    "for i in range(len(desc_vector_train)):\n",
    "    for j in range(len(feature_vector_train)):\n",
    "        tmp = desc_vector_train[i][1].copy()\n",
    "        tmp =tmp.tolist()\n",
    "        x_train.append([desc_vector_train[i][0],feature_vector_train[j][0],tmp])\n",
    "\n",
    "print(\"the length of x_train is: \",len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get test description vector\n",
    "desc_test = parse_descriptions(\"./data/descriptions_test\", num_doc=10)\n",
    "desc_vector_test = []\n",
    "for i in range(len(desc_test)):\n",
    "    tmp = doc_to_vec(desc_test[i][1], word2vec)\n",
    "    #print(tmp)\n",
    "    desc_vector_test.append([desc_test[i][0],tmp])\n",
    "\n",
    "    \n",
    "## get test feature vector\n",
    "f = open('./data/features_test/features_resnet1000_test.csv', 'r')\n",
    "feature_vector_test = []\n",
    "for line in f:\n",
    "    tmp = line.strip().split(',')\n",
    "    index = int(tmp[0][12:-4])\n",
    "    vector = [float(i) for i in tmp[1:]]\n",
    "    feature_vector_test.append([index,vector[0:dimension_of_feature_vector]])\n",
    "    #print(feature_vector_train[-1])\n",
    "f.close()\n",
    "\n",
    "# sort feature_vector_test accroding to index\n",
    "def take_index(elem):\n",
    "    return[elem[0]]\n",
    "\n",
    "#print(feature_vector_train[3][0])\n",
    "feature_vector_test.sort(key=take_index)\n",
    "\n",
    "\n",
    "\n",
    "## get x_test\n",
    "x_test = []\n",
    "for i in range(len(desc_vector_test)):\n",
    "    for j in range(len(feature_vector_test)):\n",
    "        tmp = desc_vector_test[i][1].copy()\n",
    "        tmp =tmp.tolist()\n",
    "        x_test.append([desc_vector_test[i][0],feature_vector_test[j][0],tmp])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "1000000\n",
      "20000\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_ = [a[2] for a in x_train]\n",
    "feature_dist_train_ = [b[2] for b in feature_dist_train]\n",
    "x_test_ = [c[2] for c in x_test]\n",
    "\n",
    "\n",
    "print(len(x_train_))\n",
    "print(len(feature_dist_train_))\n",
    "print(len(x_test_))\n",
    "\n",
    "\n",
    "print(len(x_test_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## linear regression\n",
    "reg = LinearRegression().fit(x_train_, feature_dist_train_)\n",
    "dissimilarity_score = reg.predict(x_test_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.23949902744832\n",
      "19.23949902744832\n"
     ]
    }
   ],
   "source": [
    "print(min(dissimilarity_score[0:2000]))\n",
    "print(max(dissimilarity_score[0:2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.23949903 19.23949903 19.23949903 ... 19.23949903 19.23949903\n",
      " 19.23949903]\n"
     ]
    }
   ],
   "source": [
    "print(dissimilarity_score[0:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
