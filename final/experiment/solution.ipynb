{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-28cb2de18188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"punkt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wordnet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import os, csv, random, re, nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "print(\"Loaded word vectors successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set()\n",
    "with open('stops.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stop_words.add(line.strip())\n",
    "total_train = 10000\n",
    "total_test = 2000\n",
    "num_train = 1000\n",
    "num_dev = 0\n",
    "num_test = 2000\n",
    "split_idx = list(range(total_train))\n",
    "random.shuffle(split_idx)\n",
    "train_idx = split_idx[:num_train]\n",
    "dev_idx = split_idx[num_train:(num_train+num_dev)]\n",
    "test_idx = list(range(total_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefixReplace(match):\n",
    "    prefix, stem = match.group(1), match.group(2)\n",
    "    temp = prefix + stem\n",
    "    if not stem in stop_words:\n",
    "        temp += ' ' + stem\n",
    "    return temp\n",
    "\n",
    "def hyphenReplace(match):\n",
    "    temp = match.group()\n",
    "    li = temp.split('-')\n",
    "    temp = temp.replace('-', '')\n",
    "    for item in li:\n",
    "        if not item in stop_words:\n",
    "            temp += ' ' + item\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    p_prefix = re.compile(r'\\b(a|an|ante|anti|auto|circum|co|com|con|contra|contro|de|dis|en|em|ex|extra|fore|hetero|homo|homeo|hyper|il|im|in|ir|inter|intra|intro|macro|micro|mid|mis|mono|non|omni|over|post|pre|pro|re|semi|sub|super|sym|syn|trans|tri|un|under|uni)-([a-z])+\\b', re.I)\n",
    "    p_hyphen = re.compile(r'\\b(\\w+-)+\\w+\\b')\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        li = []\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.lower()\n",
    "                # expand stem with hyphen prefix\n",
    "                line = p_prefix.sub(prefixReplace, line)\n",
    "                # expand hyphenated word\n",
    "                line = p_hyphen.sub(hyphenReplace, line)\n",
    "                line = line.replace(':', ' ')\n",
    "                line = line.replace('\\'s', '')\n",
    "                line = line.replace(',', ' ')\n",
    "                line = line.replace('.', ' ')\n",
    "                li += word_tokenize(line)\n",
    "                li = [wnl.lemmatize(x) for x in li]\n",
    "        docs.append(li)\n",
    "    return docs\n",
    "\n",
    "def doc_to_vec(li, word2vec):\n",
    "    # get list of word vectors in sentence\n",
    "    word_vecs = []\n",
    "    for w in li:\n",
    "        if w not in word2vec.vocab:\n",
    "#             print('not found:',w)\n",
    "            pass\n",
    "        elif w in stop_words:\n",
    "#             print('stop word:', w)\n",
    "            pass\n",
    "        else:\n",
    "            word_vecs.append(word2vec.get_vector(w))\n",
    "    if not li:\n",
    "        return np.zeros(300)\n",
    "    # return average\n",
    "    return np.stack(word_vecs).mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build description matrices\n",
    "d_train_dev = parse_descriptions(\"data/descriptions_train\", num_doc=total_train)\n",
    "test_desc = parse_descriptions(\"data/descriptions_test\", num_doc=total_test)\n",
    "d_train = np.array([doc_to_vec(d_train_dev[i], word2vec) for i in train_idx])\n",
    "d_dev = np.array([doc_to_vec(d_train_dev[i], word2vec) for i in dev_idx])\n",
    "d_test = np.array([doc_to_vec(test_desc[i], word2vec) for i in test_idx])\n",
    "d_train_dev.clear()\n",
    "test_desc.clear()\n",
    "del d_train_dev, test_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Built all description matrices!\")\n",
    "print(\"d_train shape:\", d_train.shape)\n",
    "print(\"d_dev shape:\", d_dev.shape)\n",
    "print(\"d_test shape:\", d_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test matrices\n",
    "t_train_dev = parse_descriptions(\"data/tags_train\", num_doc=total_train)\n",
    "test_tag = parse_descriptions(\"data/tags_test\", num_doc=total_test)\n",
    "t_train = np.array([doc_to_vec(t_train_dev[i], word2vec) for i in train_idx])\n",
    "t_dev = np.array([doc_to_vec(t_train_dev[i], word2vec) for i in dev_idx])\n",
    "t_test = np.array([doc_to_vec(test_tag[i], word2vec) for i in test_idx])\n",
    "\n",
    "t_train_dev.clear()\n",
    "test_tag.clear()\n",
    "del t_train_dev, test_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Built all tag matrices!\")\n",
    "print(\"t_train shape:\", t_train.shape)\n",
    "print(\"t_dev shape:\", t_dev.shape)\n",
    "print(\"t_test shape:\", t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_features(features_path):\n",
    "    vec_map = {}\n",
    "    with open(features_path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            img_id = int(row[0].split(\"/\")[1].split(\".\")[0])\n",
    "            vec_map[img_id] = np.array([float(x) for x in row[1:]])\n",
    "    return np.array([v for k, v in sorted(vec_map.items())])\n",
    "\n",
    "# build feature matrices\n",
    "# p = np.random.randn(1000, 200)\n",
    "f_train_dev = parse_features(\"data/features_train/features_resnet1000_train.csv\")# @ p\n",
    "f_train = f_train_dev[train_idx]\n",
    "f_dev = f_train_dev[dev_idx]\n",
    "f_test = parse_features(\"data/features_test/features_resnet1000_test.csv\")# @ p\n",
    "f_test = f_test[test_idx]\n",
    "\n",
    "del f_train_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Built all feature vec matrices!\")\n",
    "print(\"f_train shape:\", f_train.shape)\n",
    "print(\"f_dev shape:\", f_dev.shape)\n",
    "print(\"f_test shape:\", f_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def l2_distance(x1, x2): # euclidean distance\n",
    "    return np.linalg.norm(x1 - x2)\n",
    "\n",
    "def l1_distance(x1, x2): # manhattan distance\n",
    "    return sum(abs(x1 - x2))\n",
    "\n",
    "def build_y(vecs, closest_num, other_num):\n",
    "    y = []\n",
    "    for v1 in vecs:\n",
    "        i = 0\n",
    "        li = []\n",
    "        for v2 in vecs[:closest_num,:]:\n",
    "            heapq.heappush(li, (-l2_distance(v1, v2), i))\n",
    "            i += 1\n",
    "        for v2 in vecs[closest_num:,:]:\n",
    "            heapq.heappushpop(li, (-l2_distance(v1, v2), i))\n",
    "            i += 1\n",
    "        y += [(-d, i) for d, i in li]\n",
    "        s = set(range(len(vecs)))\n",
    "        for d, i in li:\n",
    "            s.remove(i)\n",
    "        for x in random.sample(s, other_num):\n",
    "            y.append((l2_distance(v1, vecs[x]), x))\n",
    "    return [a[0] for a in y], [a[1] for a in y]\n",
    "\n",
    "def build_x(v1, v2, num, idx):\n",
    "    x = []\n",
    "    for i in range(len(idx)):\n",
    "        x.append(np.concatenate([v1[i // num],v2[idx[i]]]))\n",
    "    return x\n",
    "\n",
    "closest_num = 10\n",
    "other_num = 10\n",
    "y_train, idx = build_y(f_train, closest_num, other_num)\n",
    "y_train = np.array(y_train)\n",
    "x_train = build_x(d_train, np.concatenate((f_train, t_train), axis=1), closest_num + other_num, idx)\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "idx.clear()\n",
    "del idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=50, n_jobs=-1)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test performance on development set\n",
    "# dev_scores = []\n",
    "# dev_pos_list = []\n",
    "# for i in range(num_dev):\n",
    "#     x_dev = []\n",
    "#     for j in range(num_dev):\n",
    "#         x_dev.append(np.concatenate((d_dev[i],f_dev[j], t_dev[j])))\n",
    "#     y_dev_pred = rf.predict(x_dev)\n",
    "#     pred_dist_idx = list(np.argsort(y_dev_pred))\n",
    "#     dev_pos = pred_dist_idx.index(i)\n",
    "#     dev_pos_list.append(dev_pos)\n",
    "#     if dev_pos < 20:\n",
    "#         dev_scores.append(1 / (dev_pos + 1))\n",
    "#     else:\n",
    "#         dev_scores.append(0.0)\n",
    "\n",
    "# print(\"Development MAP@20:\", np.mean(dev_scores))\n",
    "# print(\"Mean index of true image\", np.mean(dev_pos_list))\n",
    "# print(\"Median index of true image\", np.median(dev_pos_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"solution1_submission.csv\", \"w\")\n",
    "f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "for i in range(num_test):\n",
    "    x_test = []\n",
    "    for j in range(num_test):\n",
    "        x_test.append(np.concatenate((d_test[i],f_test[j],t_test[j])))\n",
    "    y_test_pred = rf.predict(x_test)\n",
    "    test_dist_idx = list(np.argsort(y_test_pred))\n",
    "    top_20 = test_dist_idx[:20]\n",
    "    row = [\"%d.jpg\" % i for i in top_20]\n",
    "    f.write(\"%d.txt,%s\\n\" % (i, \" \".join(row)))\n",
    "\n",
    "print(\"Output written!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
