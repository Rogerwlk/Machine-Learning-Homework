{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, csv, random, re, nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors successfully!\n"
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "print(\"Loaded word vectors successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set()\n",
    "with open('stops.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stop_words.add(line.strip())\n",
    "total_train = 10000\n",
    "total_test = 2000\n",
    "num_train = 10000\n",
    "num_dev = 0\n",
    "num_test = 2000\n",
    "split_idx = list(range(total_train))\n",
    "random.shuffle(split_idx)\n",
    "train_idx = split_idx[:num_train]\n",
    "dev_idx = split_idx[num_train:(num_train+num_dev)]\n",
    "test_idx = list(range(num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefixReplace(match):\n",
    "    prefix, stem = match.group(1), match.group(2)\n",
    "    temp = prefix + stem\n",
    "    if not stem in stop_words:\n",
    "        temp += ' ' + stem\n",
    "    return temp\n",
    "\n",
    "def hyphenReplace(match):\n",
    "    temp = match.group()\n",
    "    li = temp.split('-')\n",
    "    temp = temp.replace('-', '')\n",
    "    for item in li:\n",
    "        if not item in stop_words:\n",
    "            temp += ' ' + item\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_descriptions(data_dir, num_doc):\n",
    "    docs = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    p_prefix = re.compile(r'\\b(a|an|ante|anti|auto|circum|co|com|con|contra|contro|de|dis|en|em|ex|extra|fore|hetero|homo|homeo|hyper|il|im|in|ir|inter|intra|intro|macro|micro|mid|mis|mono|non|omni|over|post|pre|pro|re|semi|sub|super|sym|syn|trans|tri|un|under|uni)-([a-z])+\\b', re.I)\n",
    "    p_hyphen = re.compile(r'\\b(\\w+-)+\\w+\\b')\n",
    "    for i in range(num_doc):\n",
    "        path = os.path.join(data_dir, \"%d.txt\" % i)\n",
    "        li = []\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.lower()\n",
    "                # expand stem with hyphen prefix\n",
    "                line = p_prefix.sub(prefixReplace, line)\n",
    "                # expand hyphenated word\n",
    "                line = p_hyphen.sub(hyphenReplace, line)\n",
    "                line = line.replace(':', ' ')\n",
    "                line = line.replace('\\'s', '')\n",
    "                line = line.replace(',', ' ')\n",
    "                line = line.replace('.', ' ')\n",
    "                li += word_tokenize(line)\n",
    "                li = [wnl.lemmatize(x) for x in li]\n",
    "        docs.append(li)\n",
    "    return docs\n",
    "\n",
    "def doc_to_vec(li, word2vec):\n",
    "    # get list of word vectors in sentence\n",
    "    word_vecs = []\n",
    "    for w in li:\n",
    "        if w not in word2vec.vocab:\n",
    "#             print('not found:',w)\n",
    "            pass\n",
    "        elif w in stop_words:\n",
    "#             print('stop word:', w)\n",
    "            pass\n",
    "        else:\n",
    "            word_vecs.append(word2vec.get_vector(w))\n",
    "    if not li:\n",
    "        return np.zeros(300)\n",
    "    # return average\n",
    "    return np.stack(word_vecs).mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build description matrices\n",
    "d_train_dev = parse_descriptions(\"data/descriptions_train\", num_doc=total_train)\n",
    "test_desc = parse_descriptions(\"data/descriptions_test\", num_doc=total_test)\n",
    "d_train = np.array([doc_to_vec(d_train_dev[i], word2vec) for i in train_idx])\n",
    "d_dev = np.array([doc_to_vec(d_train_dev[i], word2vec) for i in dev_idx])\n",
    "d_test = np.array([doc_to_vec(test_desc[i], word2vec) for i in test_idx])\n",
    "d_train_dev.clear()\n",
    "test_desc.clear()\n",
    "del d_train_dev, test_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all description matrices!\n",
      "d_train shape: (10000, 300)\n",
      "d_dev shape: (0,)\n",
      "d_test shape: (2000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Built all description matrices!\")\n",
    "print(\"d_train shape:\", d_train.shape)\n",
    "print(\"d_dev shape:\", d_dev.shape)\n",
    "print(\"d_test shape:\", d_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test matrices\n",
    "t_train_dev = parse_descriptions(\"data/tags_train\", num_doc=total_train)\n",
    "test_tag = parse_descriptions(\"data/tags_test\", num_doc=total_test)\n",
    "t_train = np.array([doc_to_vec(t_train_dev[i], word2vec) for i in train_idx])\n",
    "t_dev = np.array([doc_to_vec(t_train_dev[i], word2vec) for i in dev_idx])\n",
    "t_test = np.array([doc_to_vec(test_tag[i], word2vec) for i in test_idx])\n",
    "\n",
    "t_train_dev.clear()\n",
    "test_tag.clear()\n",
    "del t_train_dev, test_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all tag matrices!\n",
      "t_train shape: (10000, 300)\n",
      "t_dev shape: (0,)\n",
      "t_test shape: (2000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Built all tag matrices!\")\n",
    "print(\"t_train shape:\", t_train.shape)\n",
    "print(\"t_dev shape:\", t_dev.shape)\n",
    "print(\"t_test shape:\", t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_features(features_path):\n",
    "    vec_map = {}\n",
    "    with open(features_path) as f:\n",
    "        for row in csv.reader(f):\n",
    "            img_id = int(row[0].split(\"/\")[1].split(\".\")[0])\n",
    "            vec_map[img_id] = np.array([float(x) for x in row[1:]])\n",
    "    return np.array([v for k, v in sorted(vec_map.items())])\n",
    "\n",
    "# build feature matrices\n",
    "# p = np.random.randn(1000, 200)\n",
    "f_train_dev = parse_features(\"data/features_train/features_resnet1000_train.csv\")# @ p\n",
    "f_train = f_train_dev[train_idx]\n",
    "f_dev = f_train_dev[dev_idx]\n",
    "f_test = parse_features(\"data/features_test/features_resnet1000_test.csv\")# @ p\n",
    "f_test = f_test[test_idx]\n",
    "\n",
    "del f_train_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all feature vec matrices!\n",
      "f_train shape: (10000, 1000)\n",
      "f_dev shape: (0, 1000)\n",
      "f_test shape: (2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Built all feature vec matrices!\")\n",
    "print(\"f_train shape:\", f_train.shape)\n",
    "print(\"f_dev shape:\", f_dev.shape)\n",
    "print(\"f_test shape:\", f_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_train_dev = parse_features(\"data/features_train/features_resnet1000intermediate_train.csv\")\n",
    "fi_train = fi_train_dev[train_idx]\n",
    "fi_dev = fi_train_dev[dev_idx]\n",
    "fi_test = parse_features(\"data/features_test/features_resnet1000intermediate_test.csv\")# @ p\n",
    "fi_test = fi_test[test_idx]\n",
    "\n",
    "del fi_train_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built all intermediate feature vec matrices!\n",
      "fi_train shape: (10000, 2048)\n",
      "fi_dev shape: (0, 2048)\n",
      "fi_test shape: (2000, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(\"Built all intermediate feature vec matrices!\")\n",
    "print(\"fi_train shape:\", fi_train.shape)\n",
    "print(\"fi_dev shape:\", fi_dev.shape)\n",
    "print(\"fi_test shape:\", fi_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((d_train, f_train, t_train, fi_train), axis=1)\n",
    "y_train = d_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3648) (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# lin = LinearRegression(n_jobs=-1)\n",
    "# lin.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.concatenate((d_test, f_test, t_test, fi_test), axis=1)\n",
    "y_test_pred = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "distance = cdist(d_test, y_test_pred, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written!\n"
     ]
    }
   ],
   "source": [
    "f = open(\"solution2_submission.csv\", \"w\")\n",
    "f.write(\"Descritpion_ID,Top_20_Image_IDs\\n\")\n",
    "for i in range(num_test):\n",
    "    test_dist_idx = list(np.argsort(distance[i]))\n",
    "    top_20 = test_dist_idx[:20]\n",
    "    row = [\"%d.jpg\" % i for i in top_20]\n",
    "    f.write(\"%d.txt,%s\\n\" % (i, \" \".join(row)))\n",
    "f.close()\n",
    "print(\"Output written!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
